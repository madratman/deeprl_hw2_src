\BOOKMARK [1][-]{section.1}{[5pts] Show that update 1 and update 2 are the same when the functions in Q are of the form Qw\(s,a\) = wT\(s,a\), with w R|S||A| and : S A R|S||A|, where the feature function \040is of the form \(s,a\)s',a' = 1[s'=s, a'=a]}{}% 1
\BOOKMARK [1][-]{section.2}{[5pts] Implement a linear Q-network \(no experience replay or target fixing\). Use the experimental setup of mnih2013playing,mnih2015human to the extent possible}{}% 2
\BOOKMARK [1][-]{section.3}{[10pts] Implement a linear Q-network with experience replay and target fixing. Use the experimental setup of mnih2013playing,mnih2015human to the extent possible}{}% 3
\BOOKMARK [1][-]{section.4}{[5pts] Implement a linear double Q-network. Use the the experimental setup of mnih2013playing,mnih2015human to the extent possible.}{}% 4
\BOOKMARK [1][-]{section.5}{[35pts] Implement the deep Q-network as described in mnih2013playing,mnih2015human}{}% 5
\BOOKMARK [1][-]{section.6}{[20pts] Implement the double deep Q-network as described in van2016deep}{}% 6
\BOOKMARK [1][-]{section.7}{[20pts] Implement the dueling deep Q-network as described in wang2015dueling}{}% 7
\BOOKMARK [1][-]{section.8}{Table comparing rewards for each fully trained model}{}% 8
\BOOKMARK [1][-]{section.9}{Conclusion and overall comparison of architectures}{}% 9
