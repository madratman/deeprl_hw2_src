\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}[5pts] Show that update \ref  {eq:updateQ} and update \ref  {eq:updatew} are the same when the functions in $Q$ are of the form $Q_w(s,a) = w^T\phi (s,a)$, with $w \in \mathbb  {R}^{|S||A|}$ and $\phi : S \times A \rightarrow \mathbb  {R}^{|S||A|}$, where the feature function $\phi $ is of the form $\phi (s,a)_{s',a'} = \mathbb  {1}[s'=s, a'=a]$}{1}{section.1}}
\newlabel{eq:updateQ}{{1}{1}{[5pts] Show that update \ref {eq:updateQ} and update \ref {eq:updatew} are the same when the functions in $Q$ are of the form $Q_w(s,a) = w^T\phi (s,a)$, with $w \in \mathbb {R}^{|S||A|}$ and $\phi : S \times A \rightarrow \mathbb {R}^{|S||A|}$, where the feature function $\phi $ is of the form $\phi (s,a)_{s',a'} = \mathbb {1}[s'=s, a'=a]$}{equation.1.1}{}}
\newlabel{eq:updatew}{{2}{1}{[5pts] Show that update \ref {eq:updateQ} and update \ref {eq:updatew} are the same when the functions in $Q$ are of the form $Q_w(s,a) = w^T\phi (s,a)$, with $w \in \mathbb {R}^{|S||A|}$ and $\phi : S \times A \rightarrow \mathbb {R}^{|S||A|}$, where the feature function $\phi $ is of the form $\phi (s,a)_{s',a'} = \mathbb {1}[s'=s, a'=a]$}{equation.1.2}{}}
\newlabel{eq:derivation_1}{{3}{1}{[5pts] Show that update \ref {eq:updateQ} and update \ref {eq:updatew} are the same when the functions in $Q$ are of the form $Q_w(s,a) = w^T\phi (s,a)$, with $w \in \mathbb {R}^{|S||A|}$ and $\phi : S \times A \rightarrow \mathbb {R}^{|S||A|}$, where the feature function $\phi $ is of the form $\phi (s,a)_{s',a'} = \mathbb {1}[s'=s, a'=a]$}{equation.1.3}{}}
\newlabel{eq:derivation_2}{{4}{1}{[5pts] Show that update \ref {eq:updateQ} and update \ref {eq:updatew} are the same when the functions in $Q$ are of the form $Q_w(s,a) = w^T\phi (s,a)$, with $w \in \mathbb {R}^{|S||A|}$ and $\phi : S \times A \rightarrow \mathbb {R}^{|S||A|}$, where the feature function $\phi $ is of the form $\phi (s,a)_{s',a'} = \mathbb {1}[s'=s, a'=a]$}{equation.1.4}{}}
\newlabel{eq:derivation_3}{{5}{1}{[5pts] Show that update \ref {eq:updateQ} and update \ref {eq:updatew} are the same when the functions in $Q$ are of the form $Q_w(s,a) = w^T\phi (s,a)$, with $w \in \mathbb {R}^{|S||A|}$ and $\phi : S \times A \rightarrow \mathbb {R}^{|S||A|}$, where the feature function $\phi $ is of the form $\phi (s,a)_{s',a'} = \mathbb {1}[s'=s, a'=a]$}{equation.1.5}{}}
\citation{mnih2013playing,mnih2015human}
\newlabel{eq:derivation_4}{{6}{2}{[5pts] Show that update \ref {eq:updateQ} and update \ref {eq:updatew} are the same when the functions in $Q$ are of the form $Q_w(s,a) = w^T\phi (s,a)$, with $w \in \mathbb {R}^{|S||A|}$ and $\phi : S \times A \rightarrow \mathbb {R}^{|S||A|}$, where the feature function $\phi $ is of the form $\phi (s,a)_{s',a'} = \mathbb {1}[s'=s, a'=a]$}{equation.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}[5pts] Implement a linear Q-network (no experience replay or target fixing). Use the experimental setup of \cite  {mnih2013playing,mnih2015human} to the extent possible}{2}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mean reward per episode plot for the case of linear network without target fixing and without experience replay}}{2}{figure.1}}
\newlabel{fig:r_q2}{{1}{2}{Mean reward per episode plot for the case of linear network without target fixing and without experience replay}{figure.1}{}}
\citation{mnih2013playing,mnih2015human}
\@writefile{toc}{\contentsline {section}{\numberline {3}[10pts] Implement a linear Q-network with experience replay and target fixing. Use the experimental setup of \cite  {mnih2013playing,mnih2015human} to the extent possible}{3}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Mean reward per episode plot for the case of linear network with target fixing and with experience replay}}{3}{figure.2}}
\newlabel{fig:r_q3}{{2}{3}{Mean reward per episode plot for the case of linear network with target fixing and with experience replay}{figure.2}{}}
\citation{mnih2013playing,mnih2015human}
\@writefile{toc}{\contentsline {section}{\numberline {4}[5pts] Implement a linear double Q-network. Use the the experimental setup of \cite  {mnih2013playing,mnih2015human} to the extent possible.}{4}{section.4}}
\newlabel{fig:r_q4}{{4}{4}{[5pts] Implement a linear double Q-network. Use the the experimental setup of \cite {mnih2013playing,mnih2015human} to the extent possible}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean reward per episode plot for the case of double linear network with target fixing and with experience replay}}{4}{figure.3}}
\citation{mnih2013playing,mnih2015human}
\@writefile{toc}{\contentsline {section}{\numberline {5}[35pts] Implement the deep Q-network as described in \cite  {mnih2013playing,mnih2015human}}{5}{section.5}}
\newlabel{fig:r_q5}{{5}{5}{[35pts] Implement the deep Q-network as described in \cite {mnih2013playing,mnih2015human}}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Mean reward per episode plot for Space Invaders for the case of deep Q network with target fixing and with experience replay}}{5}{figure.4}}
\citation{mnih2015human}
\citation{van2016deep}
\citation{wang2015dueling}
\@writefile{toc}{\contentsline {section}{\numberline {6}[20pts] Implement the double deep Q-network as described in \cite  {van2016deep}}{6}{section.6}}
\newlabel{fig:r_q6}{{6}{7}{[20pts] Implement the double deep Q-network as described in \cite {van2016deep}}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Mean reward per episode plot for the case of double deep network with target fixing and with experience replay}}{7}{figure.5}}
\@writefile{toc}{\contentsline {section}{\numberline {7}[20pts] Implement the dueling deep Q-network as described in \cite  {wang2015dueling}}{7}{section.7}}
\newlabel{fig:r_q7}{{7}{7}{[20pts] Implement the dueling deep Q-network as described in \cite {wang2015dueling}}{section.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Mean reward per episode plot for the case of dueling deep network with target fixing and with experience replay}}{7}{figure.6}}
\citation{wang2015dueling}
\@writefile{toc}{\contentsline {section}{\numberline {8}Table comparing rewards for each fully trained model}{8}{section.8}}
\newlabel{sec:table_comparing_rewards_for_each_fully_trained_model}{{8}{8}{Table comparing rewards for each fully trained model}{section.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Avg reward per episode for 100 episodes in implemented networks}}{8}{table.1}}
\newlabel{sample-table}{{1}{8}{Avg reward per episode for 100 episodes in implemented networks}{table.1}{}}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{mnih2013playing}{{1}{}{{}}{{}}}
\bibcite{mnih2015human}{{2}{}{{}}{{}}}
\bibcite{van2016deep}{{3}{}{{}}{{}}}
\bibcite{wang2015dueling}{{4}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion and overall comparison of architectures}{9}{section.9}}
\newlabel{sec:conclusion_and_overall_comparison_of_architectures}{{9}{9}{Conclusion and overall comparison of architectures}{section.9}{}}
\newlabel{fig:r_all}{{9}{9}{Conclusion and overall comparison of architectures}{section.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Mean reward per episode plot for all architectures. Orange: linear without target fixing and replay. Cyan: linear without target fixing and replay. Purple: double linear. Blue: deep network. Green: double-deep network. Yellow: dueling deep network}}{9}{figure.7}}
